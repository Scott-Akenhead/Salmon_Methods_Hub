---
title: "Untitled"
author: "Scott Akenhead 1-250-210-4410 scott@s4s.com"
format: pdf
editor: visual
---

# An Ontology from Methods Descriptions

The following steps, modified from those proposed by AI ([Claude 3.5](claude.ai)) are implemented to derive a prototype ontology from methods descriptions in [MonitoringResources.org](MonitoringResources.org).

## An example of AI building an ontology

The following paragraph is a sample of free text, an actual abstractText field:

| *The number of juvenile salmonids migrating downstream past a rotary screw trap in a given period is estimated using an estimate of trap efficiency and the trap catch during the period. Period estimates and their associated variances are combined over a migration season to estimate the number of migrants in a migration season with an associated confidence interval. Migrant abundance is estimated by conducting weekly trap efficiency tests throughout the migratory year at the trap site.*

Here is Claude's process of extracting key phrases and building a simple ontology from it.

**Step 1: Key Phrase Extraction**

From the given text, extract the following key phrases:

1.  Juvenile salmonids
2.  Rotary screw trap
3.  Trap efficiency
4.  Trap catch
5.  Period estimates
6.  Variances
7.  Migration season
8.  Migrants
9.  Migrant abundance
10. Trap efficiency tests
11. Migratory year
12. Trap site

**Step 2: Categorization**

Categorize these key phrases:

\- **Fish**: Juvenile salmonids\
- **Equipment**: Rotary screw trap\
- **Measurements**: Trap efficiency, Trap catch\
- **Statistical concepts**: Period estimates, Variances\
- **Time periods**: Migration season, Migratory year\
- **Subjects**: Migrants\
- **Processes**: Migrant abundance estimation, Trap efficiency tests\
- **Locations**: Trap site

**Step 3: Hierarchy Building**

A simple ontology based on these categories:

```         
Fisheries Research
|
|-- Organisms
|   |-- Fish
|       |-- Juvenile salmonids
|
|-- Equipment
|   |-- Rotary screw trap
|
|-- Measurements
|   |-- Trap efficiency
|   |-- Trap catch
|
|-- Statistical Concepts
|   |-- Period estimates
|   |-- Variances
|
|-- Time Periods
|   |-- Migration season
|   |-- Migratory year
|
|-- Research Subjects
|   |-- Migrants
|
|-- Processes
|   |-- Migrant abundance estimation
|   |-- Trap efficiency tests
|
|-- Locations
    |-- Trap site
```

This ontology provides a hierarchical structure for the key concepts in the text, organizing them from more general categories to specific instances. *I was amazed.*

**Step 4: Relationships (optional extension)**

We could further enhance this ontology by adding relationships between concepts, such as:

-   Juvenile salmonids **are_counted_by** Rotary screw trap
-   Trap efficiency **is_used_to_calculate** Migrant abundance
-   Trap efficiency tests **are_conducted_at** Trap site
-   Migrants **are_observed_during** Migration season

This example demonstrates how we can extract key phrases from a specific text and organize them into a meaningful ontology. The process can be refined and expanded for larger texts or more complex domains.

## Proceeding to actual method descriptions

The text we want to use as the basis for building a salmon methods ontology is 1,396 descriptions of methods used in environmental science, available in [MonitoringResources.org](MonitoringResources.org), a program under the auspices of the [Pacific Northwest Aquatic Monitoring Partnership](pnamp.org). This source is network of science plans that involve projects with protocols that involve methods that involve equiptment. Even confining this project only to the Abstracts of Method Descriptions will result in a large ontology.

Here's how we could proceed (adapted from suggestion by Claude3.5):

1.  Data Preparation:
    -   Ensure all descriptions are in a consistent format, ideally plain text.
    -   Organize them into a single corpus or database.
2.  Automated Key Phrase Extraction:
    -   Use Natural Language Processing (NLP) techniques to extract key phrases automatically.
    -   Focus on noun phrases, as these are likely to represent important concepts.
3.  Frequency Analysis:
    -   Analyze the frequency of extracted phrases across all descriptions.
    -   This will help identify the most common and potentially most important concepts.
4.  Domain-Specific Dictionary:
    -   Create a dictionary of environmental science terms to improve accuracy.
    -   This could be based on existing environmental science glossaries or taxonomies.
5.  Hierarchical Clustering:
    -   Use techniques like hierarchical agglomerative clustering to group similar terms.
    -   This can help in forming the initial structure of the ontology.
6.  Ontology Construction:
    -   Start with high-level categories based on the main areas of environmental science (e.g., ecology, hydrology, atmospheric science).
    -   Use the clustered terms to populate these categories and create subcategories.
7.  Relationship Mapping:
    -   Identify relationships between concepts (e.g., "is-a", "part-of", "used-in").
    -   This could be done through co-occurrence analysis or more advanced NLP techniques.
8.  Ontology Refinement – Validation and Iteration:
    -   Use ontology editing tools like Protégé to refine and visualize the ontology.
    -   Have domain experts review the ontology for accuracy and completeness.
    -   Iterate on the process, refining the ontology based on feedback.
9.  Salmon Method Hub
    -   Implement the ontology in a graph database for efficient querying and updating.

# Step 1. Data Preparation

The descriptions extracted from MR.org are in XML+HTML format with a lot of unwanted tags and unused fields. For example the field marked `abstractText`{=html} has a description we want to use, but the fields marked `ownerName`{=html} and `citation`{=html} are not relevant. The following R chunks extract the fields desired, clean unwanted strings, and assemble a corpus of plain text.

## Clean_text

this code was developed by interating with Claude3.5. My convention is to identify local (not base, not package) functions by starting with a capital letter.

``` r
#|label: clean_text
library(xml2)
library(stringr)

Clean_text <- function(text) {
# Remove any XML tags that might be present in the text
  text <- str_replace_all(text, "<[^>]+>", "")
    # regex find: begins "<", then anything but ">"
    # regex find : then at least 1 of ">" 
    # so text "<p>" and "</p>" are deleted
# Remove '\\r\\n' character sequences
  text <- str_replace_all(text, "\\\\r\\\\n", " ")
# Remove double quotes
  text <- str_replace_all(text, '\\"', "")
# Remove HTML entity codes for quotes and apostrophes
  text <- str_replace_all(text, "&ldquo;", "'")  # Left double quote
  text <- str_replace_all(text, "&rdquo;", "'")  # Right double quote
  text <- str_replace_all(text, "&rsquo;", "'")  # Right single quote (apostrophe)
  text <- str_replace_all(text, "&#39;", "'")    # Apostrophe
# Remove extra whitespace
  text <- str_squish(text)
  
  return(text)
}
```

## Test Clean_text

A sample of text to be cleaned is applied.

```{r}
#| label: test_clean_text
sample_text <- "  \"abstractText\": \"<p>The screw trap consists of a cone covered in perforated plate that is mounted on a pontoon barge (see Figure 1). This is a &ldquo;sample&rdquo; text with an apostrophe&rsquo;s and &#39;another&#39; apostrophe. Deflectors prevent humans and other debris from entering the trap (see Figure 3).</p>\\r\\n\" "
print(sample_text)
cleaned_text <- Clean_text(sample_text)
print(cleaned_text)
```

## Extract *abstractText*

Previously available is the .csv file from MR.org that identifies the Methods and has a URL to download the information about each method. Similarly for Designs, Programs, Protocols, Repositories, and Study Designs. File methods.csv (42.5k bytes) is available as methods.Rdata (11.9k bytes).

``` r
#|label: assembly
# read table that identifies the Methods
  cat('working directory:', getwd(), '\n') 
  file= './data/methods.Rdata'
  if (!file.exists(file)) stop('file not found')
  methods_id <- readRDS(file)
  cat('column names for methods_id:\n', colnames(methods_id),'\n')
    # column names for methods_id: 
    # id type status url title lastUpdated 
  Extract_abstract <- function(file_path) {
    doc=readLines(file_path)
    doc= doc[4] # observed as structure to returned HTML/XML file
      # doc <- read_xml(file_path) 
        # Error in read_xml.raw(raw, encoding = encoding, base_url = base_url, as_html = as_html,  : 
        # Start tag expected, '<' not found [4]
      # abstract <- xml_text(xml_find_first(doc, ".//abstractText"))
  if (!is.na(doc)) {
    return(Clean_text(doc))
  } else {
    return(NULL)
  }
}
  
# testing 
# file_path= "http://www.monitoringresources.org/api/v1/methods/32"
# Extract_abstract(file_path) # OK! 
    # Warning message: In readLines(file_path) :
    # incomplete final line found on 'http: ... ' 
  
process_files <- function(files_vector) {
  # files <- list.files(directory, pattern = "\\.xml$", full.names = TRUE)
  corpus <- sapply(files_vector, Extract_abstract)
  # corpus <- corpus[!sapply(corpus, is.null)] # delete missing
  return(corpus)
}

# Usage
  #xml_directory <- "path/to/your/xml/files"
files_vector = methods_id[1:5,4]
corpus <- process_files(files_vector)
  # vector of strings each named (attribute) by corresponding name in files vector.
# Save the corpus to a text file
writeLines(paste(corpus, collapse = "\n\n"), "./data/methods_corpus.txt")

cat("Processed", length(corpus), "files. Corpus saved to environmental_methods_corpus.txt\n")
```

This R script does the same thing as the Python version: 1. It defines functions to clean text and extract abstracts from XML files. 2. The `process_xml_files` function goes through all XML files in a directory, extracts the abstracts, and combines them into a vector. 3. Finally, it saves all the abstracts to a single text file, with each abstract separated by two newlines.

To use this script: 1. Make sure you have the `xml2` and `stringr` packages installed. If not, you can install them with: `R    install.packages(c("xml2", "stringr"))` 2. Modify the `xml_directory` variable to point to the directory containing your XML files. 3. Run the script. It will process all XML files in the specified directory and save the extracted abstracts to a file named `environmental_methods_corpus.txt` in your current working directory.

This approach should work well for a moderate number of XML files. If you're dealing with a very large number of files or if memory becomes an issue, you might want to consider processing the files in batches or writing to the output file incrementally. After running this script, you'll have a single text file containing all your relevant descriptions, ready for the next steps in your ontology construction process. Would you like me to explain any part of this R code in more detail, or shall we move on to discussing the next steps in processing this corpus for ontology construction?

that was impressive, this is useful R code. OK, I understand your first step. Let's look closer at your second step: Automated Key Phrase Extraction.

Certainly! I'm glad the R code is useful for you. Let's dive into the second step: Automated Key Phrase Extraction. In R, we can use various techniques and packages to extract key phrases from our corpus. Here's an approach using the `udpipe` package, which provides natural language processing tools:

1.  First, install and load the necessary packages:

``` r
install.packages(c("udpipe", "tidytext", "dplyr"))
library(udpipe)
library(tidytext)
library(dplyr)
```

2.  Download and load a pre-trained model for English:

``` r
udmodel <- udpipe_download_model(language = "english")
udmodel <- udpipe_load_model(udmodel$file_model)
```

3.  Now, let's create a function to extract key phrases from our corpus:

``` r
extract_key_phrases <- function(text, udmodel) {
  # Annotate the text
  x <- udpipe_annotate(udmodel, x = text)
  x <- as.data.frame(x)
  # Extract noun phrases
  np <- subset(x, upos %in% c("NOUN", "PROPN"))
  np <- paste(np$token, collapse = " ")
  np <- unlist(strsplit(np, "\\s+"))
  # Extract noun phrases with adjectives
  np_adj <- subset(x, upos %in% c("NOUN", "PROPN", "ADJ"))
  np_adj <- paste(np_adj$token, collapse = " ")
  np_adj <- unlist(strsplit(np_adj, "\\s+"))
  # Combine and remove duplicates
  phrases <- unique(c(np, np_adj))
  return(phrases)
}
```

4.  Apply this function to our corpus:

``` r
# Assuming 'corpus' is your vector of abstracts
all_phrases <- unlist(lapply(corpus, function(text) extract_key_phrases(text, udmodel)))

# Count phrase frequencies
phrase_freq <- table(all_phrases)
phrase_freq <- sort(phrase_freq, decreasing = TRUE)

# View the top phrases
head(phrase_freq, 20)
```

This approach does the following: 1. It uses the `udpipe` package to perform part-of-speech tagging on each abstract. 2. It extracts single nouns and proper nouns as potential key phrases. 3. It also extracts combinations of adjectives with nouns as potential multi-word key phrases. 4. It combines these and removes duplicates to create a list of key phrases for each abstract. 5. Finally, it combines all key phrases from all abstracts and calculates their frequencies.

You can adjust the `extract_key_phrases` function to be more or less inclusive based on your needs. For example, you might want to include verbs, or you might want to set a minimum frequency threshold for phrases to be considered "key".

Here are some potential enhancements:

1.  Filter out common stop words:

``` r
data("stop_words")
all_phrases <- all_phrases[!all_phrases %in% stop_words$word]
```

2.  Use bigrams or trigrams for more context:

``` r
library(tokenizers)
extract_ngrams <- function(text, n) {
  ngrams <- tokenize_ngrams(text, n = n, lowercase = TRUE)
  return(unlist(ngrams))
}
bigrams <- unlist(lapply(corpus, function(text) extract_ngrams(text, 2)))
trigrams <- unlist(lapply(corpus, function(text) extract_ngrams(text, 3)))
```

3.  Use TF-IDF (Term Frequency-Inverse Document Frequency) to identify important terms:

``` r
tidy_corpus <- tibble(id = seq_along(corpus), text = corpus)
tidy_corpus <- tidy_corpus %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tfidf <- tidy_corpus %>%
  count(id, word) %>%
  bind_tf_idf(word, id, n)

top_tfidf <- tfidf %>%
  group_by(word) %>%
  summarise(tf_idf = mean(tf_idf)) %>%
  arrange(desc(tf_idf))

head(top_tfidf, 20)
```

These methods will give you a good starting point for identifying key phrases in your corpus. The next step would be to review these phrases, potentially with domain experts, to refine the list and start organizing them into your ontology structure. Would you like me to explain any of these techniques further, or shall we move on to discussing how to start organizing these key phrases into an ontology? — … Of course! You can certainly continue this work tomorrow without losing context. When you start a new conversation, you can briefly remind me of what we've been working on. For example, you could say something like: "We were working on building an ontology from 1395 environmental science method descriptions in XML format. We've covered data extraction using R and we were discussing automated key phrase extraction. Can we continue from there?"

This will help me quickly recall our previous context and allow us to pick up where we left off. I'll do my best to continue our discussion seamlessly. Is there anything specific you'd like me to recap or summarize from our conversation today before we conclude?

<https://claude.ai/chat/af5700c7-60ea-4c16-8f2d-b0fcc53556b9>
